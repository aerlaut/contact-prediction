{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ddb0ac-b84d-4215-bdb2-3c52447f8fdf",
   "metadata": {},
   "source": [
    "# Clean up fine-tuning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "046c1d62-04a1-4066-9279-ea194b4f7865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fair-esm in c:\\users\\neil_\\anaconda3\\envs\\esm_contacts\\lib\\site-packages (2.0.0)\n",
      "Parameter name: embed_tokens.weight, Size: torch.Size([33, 320])\n",
      "Parameter name: layers.0.self_attn.k_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.0.self_attn.k_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.0.self_attn.v_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.0.self_attn.v_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.0.self_attn.q_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.0.self_attn.q_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.0.self_attn.out_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.0.self_attn.out_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.0.self_attn_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.0.self_attn_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.0.fc1.weight, Size: torch.Size([1280, 320])\n",
      "Parameter name: layers.0.fc1.bias, Size: torch.Size([1280])\n",
      "Parameter name: layers.0.fc2.weight, Size: torch.Size([320, 1280])\n",
      "Parameter name: layers.0.fc2.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.0.final_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.0.final_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.self_attn.k_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.1.self_attn.k_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.self_attn.v_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.1.self_attn.v_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.self_attn.q_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.1.self_attn.q_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.self_attn.out_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.1.self_attn.out_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.self_attn_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.1.self_attn_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.fc1.weight, Size: torch.Size([1280, 320])\n",
      "Parameter name: layers.1.fc1.bias, Size: torch.Size([1280])\n",
      "Parameter name: layers.1.fc2.weight, Size: torch.Size([320, 1280])\n",
      "Parameter name: layers.1.fc2.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.final_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.1.final_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.self_attn.k_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.2.self_attn.k_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.self_attn.v_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.2.self_attn.v_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.self_attn.q_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.2.self_attn.q_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.self_attn.out_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.2.self_attn.out_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.self_attn_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.2.self_attn_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.fc1.weight, Size: torch.Size([1280, 320])\n",
      "Parameter name: layers.2.fc1.bias, Size: torch.Size([1280])\n",
      "Parameter name: layers.2.fc2.weight, Size: torch.Size([320, 1280])\n",
      "Parameter name: layers.2.fc2.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.final_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.2.final_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.self_attn.k_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.3.self_attn.k_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.self_attn.v_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.3.self_attn.v_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.self_attn.q_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.3.self_attn.q_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.self_attn.out_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.3.self_attn.out_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.self_attn_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.3.self_attn_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.fc1.weight, Size: torch.Size([1280, 320])\n",
      "Parameter name: layers.3.fc1.bias, Size: torch.Size([1280])\n",
      "Parameter name: layers.3.fc2.weight, Size: torch.Size([320, 1280])\n",
      "Parameter name: layers.3.fc2.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.final_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.3.final_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.self_attn.k_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.4.self_attn.k_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.self_attn.v_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.4.self_attn.v_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.self_attn.q_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.4.self_attn.q_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.self_attn.out_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.4.self_attn.out_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.self_attn_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.4.self_attn_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.fc1.weight, Size: torch.Size([1280, 320])\n",
      "Parameter name: layers.4.fc1.bias, Size: torch.Size([1280])\n",
      "Parameter name: layers.4.fc2.weight, Size: torch.Size([320, 1280])\n",
      "Parameter name: layers.4.fc2.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.final_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.4.final_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.self_attn.k_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.5.self_attn.k_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.self_attn.v_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.5.self_attn.v_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.self_attn.q_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.5.self_attn.q_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.self_attn.out_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.5.self_attn.out_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.self_attn_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.5.self_attn_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.fc1.weight, Size: torch.Size([1280, 320])\n",
      "Parameter name: layers.5.fc1.bias, Size: torch.Size([1280])\n",
      "Parameter name: layers.5.fc2.weight, Size: torch.Size([320, 1280])\n",
      "Parameter name: layers.5.fc2.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.final_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.5.final_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: contact_head.regression.weight, Size: torch.Size([1, 120])\n",
      "Parameter name: contact_head.regression.bias, Size: torch.Size([1])\n",
      "Parameter name: emb_layer_norm_after.weight, Size: torch.Size([320])\n",
      "Parameter name: emb_layer_norm_after.bias, Size: torch.Size([320])\n",
      "Parameter name: lm_head.bias, Size: torch.Size([33])\n",
      "Parameter name: lm_head.dense.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: lm_head.dense.bias, Size: torch.Size([320])\n",
      "Parameter name: lm_head.layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: lm_head.layer_norm.bias, Size: torch.Size([320])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 320, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=120, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import esm\n",
    "from esm import pretrained\n",
    "\n",
    "!pip install fair-esm\n",
    "\n",
    "# Clear cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load pretrained model\n",
    "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "\n",
    "# Check layers in architecture\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}, Size: {param.size()}\")\n",
    "\n",
    "# Set device to use GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c804a73-c9dd-437c-a7d5-08f0a4ab9008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12031\n",
      "3266\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from esm.data import ESMStructuralSplitDataset\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Download structural holdout datasets (already downloaded)\n",
    "# for split_level in ['family', 'superfamily', 'fold']:\n",
    "#     for cv_partition in ['0', '1', '2', '3', '4']:\n",
    "#         esm_structural_train = ESMStructuralSplitDataset(\n",
    "#             split_level=split_level, \n",
    "#             cv_partition=cv_partition, \n",
    "#             split='train', \n",
    "#             root_path = os.path.expanduser('~/.cache/torch/data/esm'),\n",
    "#             download=True\n",
    "#         )\n",
    "#         esm_structural_valid = ESMStructuralSplitDataset(\n",
    "#             split_level=split_level, \n",
    "#             cv_partition=cv_partition, \n",
    "#             split='valid', \n",
    "#             root_path = os.path.expanduser('~/.cache/torch/data/esm'),\n",
    "#             download=True\n",
    "#         )\n",
    "\n",
    "# Set train and validation datasets from the ESM library\n",
    "esm_structural_train = ESMStructuralSplitDataset(\n",
    "    split_level='superfamily', \n",
    "    cv_partition='0', \n",
    "    split='train', \n",
    "    root_path = os.path.expanduser('~/.cache/torch/data/esm'),\n",
    ")\n",
    "\n",
    "esm_structural_valid = ESMStructuralSplitDataset(\n",
    "    split_level='superfamily', \n",
    "    cv_partition='0', \n",
    "    split='valid', \n",
    "    root_path = os.path.expanduser('~/.cache/torch/data/esm'),\n",
    ")\n",
    "\n",
    "# Check how many entries in dictionary\n",
    "print(len(esm_structural_train))\n",
    "print(len(esm_structural_valid))\n",
    "\n",
    "# Training dataset downloaded from ESMStructuralSplitDataset already split into training and validation sets\n",
    "train_dataset = esm_structural_train\n",
    "valid_dataset = esm_structural_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea5ced9d-9d1b-4f1d-9883-85b467384092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ESM-2's batch converter for tokneisation/padding\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "# Pull only the sequences in entire dataset for conversion\n",
    "train_data = [(i, train_dataset[i][\"seq\"]) for i in range(len(train_dataset))]\n",
    "# Tokenise sequences\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63fd0837-5bf8-4147-a6f8-92d722c010a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put batch token tensor in a tensor dataset object\n",
    "training_dataset = TensorDataset(batch_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8738a4-e161-4bd8-859e-8c2d0632a2e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 20, 21,  ...,  1,  1,  1],\n",
      "        [ 0,  8, 20,  ...,  1,  1,  1],\n",
      "        [ 0, 20, 16,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 0, 21, 19,  ...,  1,  1,  1],\n",
      "        [ 0, 20,  6,  ...,  1,  1,  1],\n",
      "        [ 0, 20, 10,  ...,  1,  1,  1]])\n",
      "torch.Size([12031, 884])\n",
      "<class 'torch.Tensor'>\n",
      "12031\n"
     ]
    }
   ],
   "source": [
    "# Check the batch tensor dimensions\n",
    "print(batch_tokens)\n",
    "print(batch_tokens.shape)\n",
    "print(type(batch_tokens))\n",
    "print(len(batch_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb5e8fce-adba-4781-a093-984c18ece21d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x0000020C8D173A60>\n",
      "<class 'torch.utils.data.dataset.TensorDataset'>\n",
      "12031\n"
     ]
    }
   ],
   "source": [
    "print(training_dataset)\n",
    "print(type(training_dataset))\n",
    "print(len(training_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6684f32-a8b2-4a3e-9ff7-41aafcd662cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters of the pretrained model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Last layer output fatures decided from ESM-2's layers, emb_layer_norm_after?\n",
    "hidden_size = 320\n",
    "\n",
    "# Modify last layer for our regression task\n",
    "model.final_layer = nn.Sequential(\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(in_features=hidden_size, out_features=1),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "# Enable gradient computation for the parameters in the final_layer\n",
    "for param in model.final_layer.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5901bedf-a524-4581-bd6a-751134d5da8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Removed dataloader due to dictionary and list issues in training loop, not enough memory for CUDA to process full dataset of 12031\n",
    "# # Clear cache\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# learning_rate = 0.001\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "# num_epochs = 10\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     running_loss = 0.0\n",
    "#     inputs = batch_tokens.to(device)\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(inputs)\n",
    "#     loss = loss_fn(outputs, inputs)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     running_loss += loss.item()\n",
    "#     total_loss += loss.item()\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     average_loss = total_loss / len(batch_tokens)\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {average_loss:.4f}\")\n",
    "#     print(f\"Epoch {epoch+1} loss: {running_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a0d7811-d79d-4176-b7d3-45ccbc015f29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neil_\\anaconda3\\envs\\ESM_contacts\\lib\\site-packages\\torch\\nn\\modules\\loss.py:928: UserWarning: Using a target size (torch.Size([8, 884])) that is different to the input size (torch.Size([8, 884, 33])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (33) must match the size of tensor b (884) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 42\u001b[0m\n\u001b[0;32m     33\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# what tensor to access for loss function, maybe \"logits\"?\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# What is the origin of the loss input size (torch.Size([8, 884, 33])) is this from the model embedding or lm_head?\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Parameter name: embed_tokens.weight, Size: torch.Size([33, 320])\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Parameter name: lm_head.bias, Size: torch.Size([33])\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# RuntimeError: The size of tensor a (33) must match the size of tensor b (884) at non-singleton dimension 2\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Using a target size (torch.Size([8, 884]))\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# AttributeError: 'dict' object has no attribute 'size' - I accessed logits instead, does this need to be processed i.e. softmax?\u001b[39;00m\n\u001b[0;32m     43\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ESM_contacts\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ESM_contacts\\lib\\site-packages\\torch\\nn\\modules\\loss.py:928\u001b[0m, in \u001b[0;36mSmoothL1Loss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 928\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmooth_l1_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ESM_contacts\\lib\\site-packages\\torch\\nn\\functional.py:3202\u001b[0m, in \u001b[0;36msmooth_l1_loss\u001b[1;34m(input, target, size_average, reduce, reduction, beta)\u001b[0m\n\u001b[0;32m   3199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3200\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3202\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msmooth_l1_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), beta)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ESM_contacts\\lib\\site-packages\\torch\\functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (33) must match the size of tensor b (884) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# Clear cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#loss_fn = nn.MSELoss()\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Set a low batch size for memory efficency\n",
    "batch_size = 8\n",
    "\n",
    "train_dataloader = DataLoader(batch_tokens, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_tokens in train_dataloader:\n",
    "\n",
    "        #batch_tokens = torch.stack(batch_token_list).to(device) list of tensors issue\n",
    "        \n",
    "        # Reshape tensor to 2D shape [batch_size, sequence_length] to pass assert tokens.ndim == 2\n",
    "        inputs = batch_tokens.reshape(batch_size, -1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        output_tensor = outputs[\"logits\"] # what tensor to access for loss function, maybe \"logits\"?\n",
    "        \n",
    "        # What is the origin of the loss input size (torch.Size([8, 884, 33])) is this from the model embedding or lm_head?\n",
    "        # Parameter name: embed_tokens.weight, Size: torch.Size([33, 320])\n",
    "        # Parameter name: lm_head.bias, Size: torch.Size([33])\n",
    "        \n",
    "        # RuntimeError: The size of tensor a (33) must match the size of tensor b (884) at non-singleton dimension 2\n",
    "        # Using a target size (torch.Size([8, 884]))\n",
    "        \n",
    "        loss = loss_fn(output_tensor, inputs) # AttributeError: 'dict' object has no attribute 'size' - I accessed logits instead, does this need to be processed i.e. softmax?\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {average_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch+1} loss: {running_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "932f5c39-fde5-420b-89bb-a5c7745790ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(batch_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98dfb1b1-4a85-4b7c-9018-a0f195dc4acd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 20,  6,  ...,  1,  1,  1],\n",
      "        [ 0,  6,  7,  ...,  1,  1,  1],\n",
      "        [ 0, 17,  8,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 0,  6,  8,  ...,  1,  1,  1],\n",
      "        [ 0, 20,  8,  ...,  1,  1,  1],\n",
      "        [ 0, 20, 15,  ...,  1,  1,  1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fc9b1a3-d919-4ae4-b19b-ee7f58147d48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84baf49b-64ea-41c8-9001-e775e2c11a28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 884])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6247ed13-3547-406d-84a0-96b372cf5659",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "063c0d67-376e-4b77-aad0-8eb43695603a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 884, 33])\n"
     ]
    }
   ],
   "source": [
    "print(outputs[\"logits\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "903a1300-a20f-49f9-bc07-86335400fbf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logits': tensor([[[ 14.2351,  -7.3852,  -6.2489,  ..., -15.6001, -15.7696,  -7.3804],\n",
      "         [ -7.4059, -15.0762,  -7.7357,  ..., -15.8675, -16.0993, -15.0751],\n",
      "         [-11.7911, -19.6771, -10.7796,  ..., -16.2037, -16.2054, -19.6660],\n",
      "         ...,\n",
      "         [-10.3239, -17.0342, -10.4923,  ..., -16.2025, -16.2505, -17.0353],\n",
      "         [-10.8626, -18.0419, -11.0570,  ..., -16.2881, -16.3453, -18.0377],\n",
      "         [-10.8497, -18.7546, -11.6085,  ..., -16.3317, -16.3946, -18.7504]],\n",
      "\n",
      "        [[ 15.3661,  -8.9175,  -6.0069,  ..., -15.3930, -15.5476,  -8.9246],\n",
      "         [ -7.3944, -15.1793,  -6.9476,  ..., -15.6827, -15.9010, -15.1780],\n",
      "         [-11.0365, -21.0386, -11.8327,  ..., -16.4465, -16.4803, -21.0372],\n",
      "         ...,\n",
      "         [-10.7051, -22.7104, -10.0268,  ..., -16.1865, -16.0903, -22.6981],\n",
      "         [-12.1272, -23.1866, -11.0768,  ..., -16.1650, -16.1004, -23.1894],\n",
      "         [-11.4274, -20.6906, -11.0352,  ..., -16.3272, -16.3195, -20.6995]],\n",
      "\n",
      "        [[ 15.1409,  -7.3800,  -6.3392,  ..., -15.7751, -15.9175,  -7.3630],\n",
      "         [ -7.0562, -17.2873,  -7.4785,  ..., -15.4599, -15.8634, -17.2807],\n",
      "         [-12.1553, -20.7590, -12.1886,  ..., -16.2713, -16.4757, -20.7458],\n",
      "         ...,\n",
      "         [ -9.1377, -18.2674,  -9.8503,  ..., -16.3767, -16.3937, -18.2594],\n",
      "         [ -9.1375, -18.2942,  -9.5004,  ..., -16.3589, -16.3910, -18.2750],\n",
      "         [-11.3009, -20.4799, -10.5267,  ..., -16.3222, -16.3355, -20.4549]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 14.9447,  -8.2206,  -6.3125,  ..., -15.6045, -15.7326,  -8.2256],\n",
      "         [ -6.8806, -14.7277,  -7.6904,  ..., -15.8295, -16.0558, -14.7268],\n",
      "         [-11.3938, -20.7254, -11.7880,  ..., -16.6348, -16.6909, -20.7199],\n",
      "         ...,\n",
      "         [-11.2966, -19.9130, -11.0983,  ..., -16.2725, -16.2737, -19.9082],\n",
      "         [-11.6034, -20.1539, -11.3951,  ..., -16.2327, -16.2585, -20.1485],\n",
      "         [-11.2236, -20.2068, -11.3057,  ..., -16.2854, -16.3019, -20.2066]],\n",
      "\n",
      "        [[ 15.4551,  -5.7811,  -5.2220,  ..., -15.3973, -15.5893,  -5.7694],\n",
      "         [ -8.9538, -14.4900,  -9.0114,  ..., -15.8785, -16.0705, -14.4804],\n",
      "         [-11.8573, -20.1112,  -9.6548,  ..., -16.4026, -16.4960, -20.0951],\n",
      "         ...,\n",
      "         [-11.5615, -19.0621, -10.5073,  ..., -16.1311, -16.2332, -19.0541],\n",
      "         [-10.1884, -18.4736, -10.8202,  ..., -15.6633, -15.6830, -18.4628],\n",
      "         [-11.7326, -18.8199, -10.5263,  ..., -15.9494, -16.0477, -18.8187]],\n",
      "\n",
      "        [[ 16.3267,  -8.9463,  -7.2571,  ..., -15.1161, -15.3325,  -8.9337],\n",
      "         [ -7.7886, -15.4909,  -7.1368,  ..., -15.5989, -15.8357, -15.5024],\n",
      "         [-12.6288, -23.1090, -12.2002,  ..., -16.1703, -16.1098, -23.0935],\n",
      "         ...,\n",
      "         [-13.2199, -25.3788, -13.0310,  ..., -16.5294, -16.5059, -25.3985],\n",
      "         [-12.1629, -24.7392, -11.9712,  ..., -16.4177, -16.4165, -24.7534],\n",
      "         [-11.6075, -24.6721, -12.0428,  ..., -16.3596, -16.3572, -24.6889]]],\n",
      "       device='cuda:0'), 'representations': {}}\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612270bb-81e7-439e-b216-58d8772c10fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649555b-31ff-40bf-9d98-784d5aee8c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESM-2 tokeniser for data\n",
    "batch_size = 8\n",
    "\n",
    "valid_data = [(i, valid_dataset[i][\"seq\"]) for i in range(len(valid_dataset))]\n",
    "v_batch_labels, v_batch_strs, v_batch_tokens = batch_converter(valid_data)\n",
    "valid_dataloader = DataLoader(v_batch_tokens, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set evaluation mode for dropout and batch normalisation\n",
    "model.eval()\n",
    "\n",
    "# Initialise lists   \n",
    "predictions = []\n",
    "true_contacts = []\n",
    "\n",
    "# Turn off gradients for evaluation\n",
    "with torch.no_grad():\n",
    "    for batch in valid_dataloader:\n",
    "        inputs = v_batch_tokens.to(device)  # Move batch to the specified device\n",
    "        outputs = model(inputs)\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "# Convert predictions and contact lists into tensors\n",
    "predictions = torch.tensor(predictions)\n",
    "true_contacts = torch.tensor(targets)\n",
    "\n",
    "# Evaluation metrics\n",
    "accuracy = accuracy_score(true_contacts, predictions)\n",
    "precision = precision_score(true_contacts, predictions)\n",
    "recall = recall_score(true_contacts, predictions)\n",
    "f1 = f1_score(true_contacts, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
