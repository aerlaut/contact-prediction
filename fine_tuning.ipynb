{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ddb0ac-b84d-4215-bdb2-3c52447f8fdf",
   "metadata": {},
   "source": [
    "# Clean up fine-tuning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "046c1d62-04a1-4066-9279-ea194b4f7865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fair-esm in c:\\users\\neil_\\anaconda3\\envs\\esm_contacts\\lib\\site-packages (2.0.0)\n",
      "Parameter name: embed_tokens.weight, Size: torch.Size([33, 320])\n",
      "Parameter name: layers.0.self_attn.k_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.0.self_attn.k_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.0.self_attn.v_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.0.self_attn.v_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.0.self_attn.q_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.0.self_attn.q_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.0.self_attn.out_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.0.self_attn.out_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.0.self_attn_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.0.self_attn_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.0.fc1.weight, Size: torch.Size([1280, 320])\n",
      "Parameter name: layers.0.fc1.bias, Size: torch.Size([1280])\n",
      "Parameter name: layers.0.fc2.weight, Size: torch.Size([320, 1280])\n",
      "Parameter name: layers.0.fc2.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.0.final_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.0.final_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.self_attn.k_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.1.self_attn.k_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.self_attn.v_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.1.self_attn.v_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.self_attn.q_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.1.self_attn.q_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.self_attn.out_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.1.self_attn.out_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.self_attn_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.1.self_attn_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.fc1.weight, Size: torch.Size([1280, 320])\n",
      "Parameter name: layers.1.fc1.bias, Size: torch.Size([1280])\n",
      "Parameter name: layers.1.fc2.weight, Size: torch.Size([320, 1280])\n",
      "Parameter name: layers.1.fc2.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.final_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.1.final_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.self_attn.k_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.2.self_attn.k_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.self_attn.v_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.2.self_attn.v_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.self_attn.q_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.2.self_attn.q_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.self_attn.out_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.2.self_attn.out_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.self_attn_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.2.self_attn_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.fc1.weight, Size: torch.Size([1280, 320])\n",
      "Parameter name: layers.2.fc1.bias, Size: torch.Size([1280])\n",
      "Parameter name: layers.2.fc2.weight, Size: torch.Size([320, 1280])\n",
      "Parameter name: layers.2.fc2.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.final_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.2.final_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.self_attn.k_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.3.self_attn.k_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.self_attn.v_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.3.self_attn.v_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.self_attn.q_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.3.self_attn.q_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.self_attn.out_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.3.self_attn.out_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.self_attn_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.3.self_attn_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.fc1.weight, Size: torch.Size([1280, 320])\n",
      "Parameter name: layers.3.fc1.bias, Size: torch.Size([1280])\n",
      "Parameter name: layers.3.fc2.weight, Size: torch.Size([320, 1280])\n",
      "Parameter name: layers.3.fc2.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.final_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.3.final_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.self_attn.k_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.4.self_attn.k_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.self_attn.v_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.4.self_attn.v_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.self_attn.q_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.4.self_attn.q_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.self_attn.out_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.4.self_attn.out_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.self_attn_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.4.self_attn_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.fc1.weight, Size: torch.Size([1280, 320])\n",
      "Parameter name: layers.4.fc1.bias, Size: torch.Size([1280])\n",
      "Parameter name: layers.4.fc2.weight, Size: torch.Size([320, 1280])\n",
      "Parameter name: layers.4.fc2.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.final_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.4.final_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.self_attn.k_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.5.self_attn.k_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.self_attn.v_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.5.self_attn.v_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.self_attn.q_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.5.self_attn.q_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.self_attn.out_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.5.self_attn.out_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.self_attn_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.5.self_attn_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.fc1.weight, Size: torch.Size([1280, 320])\n",
      "Parameter name: layers.5.fc1.bias, Size: torch.Size([1280])\n",
      "Parameter name: layers.5.fc2.weight, Size: torch.Size([320, 1280])\n",
      "Parameter name: layers.5.fc2.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.final_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.5.final_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: contact_head.regression.weight, Size: torch.Size([1, 120])\n",
      "Parameter name: contact_head.regression.bias, Size: torch.Size([1])\n",
      "Parameter name: emb_layer_norm_after.weight, Size: torch.Size([320])\n",
      "Parameter name: emb_layer_norm_after.bias, Size: torch.Size([320])\n",
      "Parameter name: lm_head.bias, Size: torch.Size([33])\n",
      "Parameter name: lm_head.dense.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: lm_head.dense.bias, Size: torch.Size([320])\n",
      "Parameter name: lm_head.layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: lm_head.layer_norm.bias, Size: torch.Size([320])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 320, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=120, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import esm\n",
    "from esm import pretrained\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n",
    "\n",
    "!pip install fair-esm\n",
    "\n",
    "# Clear cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load pretrained model\n",
    "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "\n",
    "# Check layers in architecture\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}, Size: {param.size()}\")\n",
    "\n",
    "# Set device to use GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c804a73-c9dd-437c-a7d5-08f0a4ab9008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12031\n",
      "3266\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from esm.data import ESMStructuralSplitDataset\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Download structural holdout datasets (already downloaded)\n",
    "# for split_level in ['family', 'superfamily', 'fold']:\n",
    "#     for cv_partition in ['0', '1', '2', '3', '4']:\n",
    "#         esm_structural_train = ESMStructuralSplitDataset(\n",
    "#             split_level=split_level, \n",
    "#             cv_partition=cv_partition, \n",
    "#             split='train', \n",
    "#             root_path = os.path.expanduser('~/.cache/torch/data/esm'),\n",
    "#             download=True\n",
    "#         )\n",
    "#         esm_structural_valid = ESMStructuralSplitDataset(\n",
    "#             split_level=split_level, \n",
    "#             cv_partition=cv_partition, \n",
    "#             split='valid', \n",
    "#             root_path = os.path.expanduser('~/.cache/torch/data/esm'),\n",
    "#             download=True\n",
    "#         )\n",
    "\n",
    "# Set train and validation datasets from the ESM library\n",
    "esm_structural_train = ESMStructuralSplitDataset(\n",
    "    split_level='superfamily', \n",
    "    cv_partition='0', \n",
    "    split='train', \n",
    "    root_path = os.path.expanduser('~/.cache/torch/data/esm'),\n",
    ")\n",
    "\n",
    "esm_structural_valid = ESMStructuralSplitDataset(\n",
    "    split_level='superfamily', \n",
    "    cv_partition='0', \n",
    "    split='valid', \n",
    "    root_path = os.path.expanduser('~/.cache/torch/data/esm'),\n",
    ")\n",
    "\n",
    "# Check how many entries in dictionary\n",
    "print(len(esm_structural_train))\n",
    "print(len(esm_structural_valid))\n",
    "\n",
    "# Training dataset downloaded from ESMStructuralSplitDataset already split into training and validation sets\n",
    "train_dataset = esm_structural_train\n",
    "valid_dataset = esm_structural_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea5ced9d-9d1b-4f1d-9883-85b467384092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ESM-2's batch converter for tokneisation/padding\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "# Pull only the sequences in entire dataset for conversion\n",
    "train_data = [(i, train_dataset[i][\"seq\"]) for i in range(len(train_dataset))]\n",
    "# Tokenise sequences\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63fd0837-5bf8-4147-a6f8-92d722c010a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put batch token tensor in a tensor dataset object\n",
    "training_dataset = TensorDataset(batch_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8738a4-e161-4bd8-859e-8c2d0632a2e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 20, 21,  ...,  1,  1,  1],\n",
      "        [ 0,  8, 20,  ...,  1,  1,  1],\n",
      "        [ 0, 20, 16,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 0, 21, 19,  ...,  1,  1,  1],\n",
      "        [ 0, 20,  6,  ...,  1,  1,  1],\n",
      "        [ 0, 20, 10,  ...,  1,  1,  1]])\n",
      "torch.Size([12031, 884])\n",
      "<class 'torch.Tensor'>\n",
      "12031\n"
     ]
    }
   ],
   "source": [
    "# Check the batch tensor dimensions\n",
    "print(batch_tokens)\n",
    "print(batch_tokens.shape)\n",
    "print(type(batch_tokens))\n",
    "print(len(batch_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb5e8fce-adba-4781-a093-984c18ece21d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x0000023E24A1B880>\n",
      "<class 'torch.utils.data.dataset.TensorDataset'>\n",
      "12031\n"
     ]
    }
   ],
   "source": [
    "print(training_dataset)\n",
    "print(type(training_dataset))\n",
    "print(len(training_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb2577c9-6410-429a-bead-6ff68bcb4689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # Not sure whether to use token_representaions for output in the loss function?\n",
    "# with torch.no_grad():\n",
    "#     results = model(batch_tokens.to(device), repr_layers=[33], return_contacts=True)\n",
    "# token_representations = results[\"representations\"][33].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6684f32-a8b2-4a3e-9ff7-41aafcd662cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters of the pretrained model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Last layer output fatures decided from ESM-2's layers, emb_layer_norm_after?\n",
    "hidden_size = 320\n",
    "\n",
    "# Modify last layer for our regression task\n",
    "model.final_layer = nn.Sequential(\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(in_features=hidden_size, out_features=1),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "# Enable gradient computation for the parameters in the final_layer\n",
    "for param in model.final_layer.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5901bedf-a524-4581-bd6a-751134d5da8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Removed dataloader due to dictionary and list issues in training loop, not enough memory for CUDA to process full dataset of 12031\n",
    "# # Clear cache\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# learning_rate = 0.001\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "# num_epochs = 10\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     running_loss = 0.0\n",
    "#     inputs = batch_tokens.to(device)\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(inputs)\n",
    "#     loss = loss_fn(outputs, inputs)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     running_loss += loss.item()\n",
    "#     total_loss += loss.item()\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     average_loss = total_loss / len(batch_tokens)\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {average_loss:.4f}\")\n",
    "#     print(f\"Epoch {epoch+1} loss: {running_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a0d7811-d79d-4176-b7d3-45ccbc015f29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.40 GiB (GPU 0; 4.00 GiB total capacity; 2.85 GiB already allocated; 12.15 MiB free; 2.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 34\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepr_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m33\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_contacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m outputs \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepresentations\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m33\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#outputs = model(inputs)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#output_tensor = outputs[\"logits\"] # what tensor to access for loss function, maybe \"logits\"?\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m \n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m#loss = loss_fn(output_tensor, inputs) # AttributeError: 'dict' object has no attribute 'size' - I accessed logits instead, does this need to be processed i.e. softmax?\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ESM_contacts\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ESM_contacts\\lib\\site-packages\\esm\\model\\esm2.py:138\u001b[0m, in \u001b[0;36mESM2.forward\u001b[1;34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[0m\n\u001b[0;32m    136\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m padding_mask\u001b[38;5;241m.\u001b[39mtype_as(attentions)\n\u001b[0;32m    137\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 138\u001b[0m     attentions \u001b[38;5;241m=\u001b[39m \u001b[43mattentions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    139\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattentions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m attentions\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_contacts:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.40 GiB (GPU 0; 4.00 GiB total capacity; 2.85 GiB already allocated; 12.15 MiB free; 2.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Clear cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#loss_fn = nn.MSELoss()\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Set a low batch size for memory efficency\n",
    "batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader(batch_tokens, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_tokens in train_dataloader:\n",
    "\n",
    "        #batch_tokens = torch.stack(batch_token_list).to(device) list of tensors issue\n",
    "        \n",
    "        # Reshape tensor to 2D shape [batch_size, sequence_length] to pass assert tokens.ndim == 2\n",
    "        inputs = batch_tokens.reshape(batch_size, -1).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Access contact representations in dictionary for output? \n",
    "        with torch.no_grad():\n",
    "            results = model(inputs, repr_layers=[33], return_contacts=True)\n",
    "        \n",
    "        outputs = results[\"representations\"][33].to(device)\n",
    "        \n",
    "        #outputs = model(inputs)\n",
    "        #output_tensor = outputs[\"logits\"] # what tensor to access for loss function, maybe \"logits\"?\n",
    "        \n",
    "        # What is the origin of the loss input size (torch.Size([8, 884, 33])) is this from the model embedding or lm_head?\n",
    "        # Parameter name: embed_tokens.weight, Size: torch.Size([33, 320])\n",
    "        # Parameter name: lm_head.bias, Size: torch.Size([33])\n",
    "        # RuntimeError: The size of tensor a (33) must match the size of tensor b (884) at non-singleton dimension 2\n",
    "        # Using a target size (torch.Size([8, 884]))\n",
    "        \n",
    "        #loss = loss_fn(output_tensor, inputs) # AttributeError: 'dict' object has no attribute 'size' - I accessed logits instead, does this need to be processed i.e. softmax?\n",
    "        \n",
    "        loss = loss_fn(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {average_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch+1} loss: {running_loss:.4f}\")\n",
    "    \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "932f5c39-fde5-420b-89bb-a5c7745790ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(batch_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98dfb1b1-4a85-4b7c-9018-a0f195dc4acd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 17, 15,  ...,  1,  1,  1],\n",
      "        [ 0, 16, 20,  ...,  1,  1,  1],\n",
      "        [ 0, 14, 16,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 0, 20, 19,  ...,  1,  1,  1],\n",
      "        [ 0, 12,  7,  ...,  1,  1,  1],\n",
      "        [ 0, 11, 12,  ...,  1,  1,  1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fc9b1a3-d919-4ae4-b19b-ee7f58147d48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84baf49b-64ea-41c8-9001-e775e2c11a28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 884])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6247ed13-3547-406d-84a0-96b372cf5659",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "063c0d67-376e-4b77-aad0-8eb43695603a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 884, 33])\n"
     ]
    }
   ],
   "source": [
    "print(outputs[\"logits\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "903a1300-a20f-49f9-bc07-86335400fbf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logits': tensor([[[ 14.2351,  -7.3852,  -6.2489,  ..., -15.6001, -15.7696,  -7.3804],\n",
      "         [ -7.4059, -15.0762,  -7.7357,  ..., -15.8675, -16.0993, -15.0751],\n",
      "         [-11.7911, -19.6771, -10.7796,  ..., -16.2037, -16.2054, -19.6660],\n",
      "         ...,\n",
      "         [-10.3239, -17.0342, -10.4923,  ..., -16.2025, -16.2505, -17.0353],\n",
      "         [-10.8626, -18.0419, -11.0570,  ..., -16.2881, -16.3453, -18.0377],\n",
      "         [-10.8497, -18.7546, -11.6085,  ..., -16.3317, -16.3946, -18.7504]],\n",
      "\n",
      "        [[ 15.3661,  -8.9175,  -6.0069,  ..., -15.3930, -15.5476,  -8.9246],\n",
      "         [ -7.3944, -15.1793,  -6.9476,  ..., -15.6827, -15.9010, -15.1780],\n",
      "         [-11.0365, -21.0386, -11.8327,  ..., -16.4465, -16.4803, -21.0372],\n",
      "         ...,\n",
      "         [-10.7051, -22.7104, -10.0268,  ..., -16.1865, -16.0903, -22.6981],\n",
      "         [-12.1272, -23.1866, -11.0768,  ..., -16.1650, -16.1004, -23.1894],\n",
      "         [-11.4274, -20.6906, -11.0352,  ..., -16.3272, -16.3195, -20.6995]],\n",
      "\n",
      "        [[ 15.1409,  -7.3800,  -6.3392,  ..., -15.7751, -15.9175,  -7.3630],\n",
      "         [ -7.0562, -17.2873,  -7.4785,  ..., -15.4599, -15.8634, -17.2807],\n",
      "         [-12.1553, -20.7590, -12.1886,  ..., -16.2713, -16.4757, -20.7458],\n",
      "         ...,\n",
      "         [ -9.1377, -18.2674,  -9.8503,  ..., -16.3767, -16.3937, -18.2594],\n",
      "         [ -9.1375, -18.2942,  -9.5004,  ..., -16.3589, -16.3910, -18.2750],\n",
      "         [-11.3009, -20.4799, -10.5267,  ..., -16.3222, -16.3355, -20.4549]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 14.9447,  -8.2206,  -6.3125,  ..., -15.6045, -15.7326,  -8.2256],\n",
      "         [ -6.8806, -14.7277,  -7.6904,  ..., -15.8295, -16.0558, -14.7268],\n",
      "         [-11.3938, -20.7254, -11.7880,  ..., -16.6348, -16.6909, -20.7199],\n",
      "         ...,\n",
      "         [-11.2966, -19.9130, -11.0983,  ..., -16.2725, -16.2737, -19.9082],\n",
      "         [-11.6034, -20.1539, -11.3951,  ..., -16.2327, -16.2585, -20.1485],\n",
      "         [-11.2236, -20.2068, -11.3057,  ..., -16.2854, -16.3019, -20.2066]],\n",
      "\n",
      "        [[ 15.4551,  -5.7811,  -5.2220,  ..., -15.3973, -15.5893,  -5.7694],\n",
      "         [ -8.9538, -14.4900,  -9.0114,  ..., -15.8785, -16.0705, -14.4804],\n",
      "         [-11.8573, -20.1112,  -9.6548,  ..., -16.4026, -16.4960, -20.0951],\n",
      "         ...,\n",
      "         [-11.5615, -19.0621, -10.5073,  ..., -16.1311, -16.2332, -19.0541],\n",
      "         [-10.1884, -18.4736, -10.8202,  ..., -15.6633, -15.6830, -18.4628],\n",
      "         [-11.7326, -18.8199, -10.5263,  ..., -15.9494, -16.0477, -18.8187]],\n",
      "\n",
      "        [[ 16.3267,  -8.9463,  -7.2571,  ..., -15.1161, -15.3325,  -8.9337],\n",
      "         [ -7.7886, -15.4909,  -7.1368,  ..., -15.5989, -15.8357, -15.5024],\n",
      "         [-12.6288, -23.1090, -12.2002,  ..., -16.1703, -16.1098, -23.0935],\n",
      "         ...,\n",
      "         [-13.2199, -25.3788, -13.0310,  ..., -16.5294, -16.5059, -25.3985],\n",
      "         [-12.1629, -24.7392, -11.9712,  ..., -16.4177, -16.4165, -24.7534],\n",
      "         [-11.6075, -24.6721, -12.0428,  ..., -16.3596, -16.3572, -24.6889]]],\n",
      "       device='cuda:0'), 'representations': {}}\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649555b-31ff-40bf-9d98-784d5aee8c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESM-2 tokeniser for data\n",
    "batch_size = 8\n",
    "\n",
    "valid_data = [(i, valid_dataset[i][\"seq\"]) for i in range(len(valid_dataset))]\n",
    "v_batch_labels, v_batch_strs, v_batch_tokens = batch_converter(valid_data)\n",
    "valid_dataloader = DataLoader(v_batch_tokens, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set evaluation mode for dropout and batch normalisation\n",
    "model.eval()\n",
    "\n",
    "# Initialise lists   \n",
    "predictions = []\n",
    "true_contacts = []\n",
    "\n",
    "# Turn off gradients for evaluation\n",
    "with torch.no_grad():\n",
    "    for batch in valid_dataloader:\n",
    "        inputs = v_batch_tokens.to(device)  # Move batch to the specified device\n",
    "        outputs = model(inputs)\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "# Convert predictions and contact lists into tensors\n",
    "predictions = torch.tensor(predictions)\n",
    "true_contacts = torch.tensor(targets)\n",
    "\n",
    "# Evaluation metrics\n",
    "accuracy = accuracy_score(true_contacts, predictions)\n",
    "precision = precision_score(true_contacts, predictions)\n",
    "recall = recall_score(true_contacts, predictions)\n",
    "f1 = f1_score(true_contacts, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
