{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ddb0ac-b84d-4215-bdb2-3c52447f8fdf",
   "metadata": {},
   "source": [
    "# Clean up fine-tuning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "046c1d62-04a1-4066-9279-ea194b4f7865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fair-esm in c:\\users\\neil_\\anaconda3\\envs\\esm_contacts\\lib\\site-packages (2.0.0)\n",
      "Parameter name: embed_tokens.weight, Size: torch.Size([33, 320])\n",
      "Parameter name: layers.0.self_attn.k_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.0.self_attn.k_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.0.self_attn.v_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.0.self_attn.v_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.0.self_attn.q_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.0.self_attn.q_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.0.self_attn.out_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.0.self_attn.out_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.0.self_attn_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.0.self_attn_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.0.fc1.weight, Size: torch.Size([1280, 320])\n",
      "Parameter name: layers.0.fc1.bias, Size: torch.Size([1280])\n",
      "Parameter name: layers.0.fc2.weight, Size: torch.Size([320, 1280])\n",
      "Parameter name: layers.0.fc2.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.0.final_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.0.final_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.self_attn.k_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.1.self_attn.k_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.self_attn.v_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.1.self_attn.v_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.self_attn.q_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.1.self_attn.q_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.self_attn.out_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.1.self_attn.out_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.self_attn_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.1.self_attn_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.fc1.weight, Size: torch.Size([1280, 320])\n",
      "Parameter name: layers.1.fc1.bias, Size: torch.Size([1280])\n",
      "Parameter name: layers.1.fc2.weight, Size: torch.Size([320, 1280])\n",
      "Parameter name: layers.1.fc2.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.1.final_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.1.final_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.self_attn.k_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.2.self_attn.k_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.self_attn.v_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.2.self_attn.v_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.self_attn.q_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.2.self_attn.q_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.self_attn.out_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.2.self_attn.out_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.self_attn_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.2.self_attn_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.fc1.weight, Size: torch.Size([1280, 320])\n",
      "Parameter name: layers.2.fc1.bias, Size: torch.Size([1280])\n",
      "Parameter name: layers.2.fc2.weight, Size: torch.Size([320, 1280])\n",
      "Parameter name: layers.2.fc2.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.2.final_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.2.final_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.self_attn.k_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.3.self_attn.k_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.self_attn.v_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.3.self_attn.v_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.self_attn.q_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.3.self_attn.q_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.self_attn.out_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.3.self_attn.out_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.self_attn_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.3.self_attn_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.fc1.weight, Size: torch.Size([1280, 320])\n",
      "Parameter name: layers.3.fc1.bias, Size: torch.Size([1280])\n",
      "Parameter name: layers.3.fc2.weight, Size: torch.Size([320, 1280])\n",
      "Parameter name: layers.3.fc2.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.3.final_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.3.final_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.self_attn.k_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.4.self_attn.k_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.self_attn.v_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.4.self_attn.v_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.self_attn.q_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.4.self_attn.q_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.self_attn.out_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.4.self_attn.out_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.self_attn_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.4.self_attn_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.fc1.weight, Size: torch.Size([1280, 320])\n",
      "Parameter name: layers.4.fc1.bias, Size: torch.Size([1280])\n",
      "Parameter name: layers.4.fc2.weight, Size: torch.Size([320, 1280])\n",
      "Parameter name: layers.4.fc2.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.4.final_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.4.final_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.self_attn.k_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.5.self_attn.k_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.self_attn.v_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.5.self_attn.v_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.self_attn.q_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.5.self_attn.q_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.self_attn.out_proj.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: layers.5.self_attn.out_proj.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.self_attn_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.5.self_attn_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.fc1.weight, Size: torch.Size([1280, 320])\n",
      "Parameter name: layers.5.fc1.bias, Size: torch.Size([1280])\n",
      "Parameter name: layers.5.fc2.weight, Size: torch.Size([320, 1280])\n",
      "Parameter name: layers.5.fc2.bias, Size: torch.Size([320])\n",
      "Parameter name: layers.5.final_layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: layers.5.final_layer_norm.bias, Size: torch.Size([320])\n",
      "Parameter name: contact_head.regression.weight, Size: torch.Size([1, 120])\n",
      "Parameter name: contact_head.regression.bias, Size: torch.Size([1])\n",
      "Parameter name: emb_layer_norm_after.weight, Size: torch.Size([320])\n",
      "Parameter name: emb_layer_norm_after.bias, Size: torch.Size([320])\n",
      "Parameter name: lm_head.bias, Size: torch.Size([33])\n",
      "Parameter name: lm_head.dense.weight, Size: torch.Size([320, 320])\n",
      "Parameter name: lm_head.dense.bias, Size: torch.Size([320])\n",
      "Parameter name: lm_head.layer_norm.weight, Size: torch.Size([320])\n",
      "Parameter name: lm_head.layer_norm.bias, Size: torch.Size([320])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 320, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=120, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import esm\n",
    "from esm import pretrained\n",
    "\n",
    "!pip install fair-esm\n",
    "\n",
    "# Clear cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Set device to use GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pretrained model\n",
    "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "\n",
    "# Check layers in architecture\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}, Size: {param.size()}\")\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c804a73-c9dd-437c-a7d5-08f0a4ab9008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12031\n",
      "3266\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from esm.data import ESMStructuralSplitDataset\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Download structural holdout datasets (already downloaded)\n",
    "# for split_level in ['family', 'superfamily', 'fold']:\n",
    "#     for cv_partition in ['0', '1', '2', '3', '4']:\n",
    "#         esm_structural_train = ESMStructuralSplitDataset(\n",
    "#             split_level=split_level, \n",
    "#             cv_partition=cv_partition, \n",
    "#             split='train', \n",
    "#             root_path = os.path.expanduser('~/.cache/torch/data/esm'),\n",
    "#             download=True\n",
    "#         )\n",
    "#         esm_structural_valid = ESMStructuralSplitDataset(\n",
    "#             split_level=split_level, \n",
    "#             cv_partition=cv_partition, \n",
    "#             split='valid', \n",
    "#             root_path = os.path.expanduser('~/.cache/torch/data/esm'),\n",
    "#             download=True\n",
    "#         )\n",
    "\n",
    "# Set train and validation datasets from the ESM library\n",
    "esm_structural_train = ESMStructuralSplitDataset(\n",
    "    split_level='superfamily', \n",
    "    cv_partition='0', \n",
    "    split='train', \n",
    "    root_path = os.path.expanduser('~/.cache/torch/data/esm'),\n",
    ")\n",
    "\n",
    "esm_structural_valid = ESMStructuralSplitDataset(\n",
    "    split_level='superfamily', \n",
    "    cv_partition='0', \n",
    "    split='valid', \n",
    "    root_path = os.path.expanduser('~/.cache/torch/data/esm'),\n",
    ")\n",
    "\n",
    "# Check how many entries in dictionary\n",
    "print(len(esm_structural_train))\n",
    "print(len(esm_structural_valid))\n",
    "\n",
    "# Training dataset downloaded from ESMStructuralSplitDataset already split into training and validation sets\n",
    "train_dataset = esm_structural_train\n",
    "valid_dataset = esm_structural_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea5ced9d-9d1b-4f1d-9883-85b467384092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 20, 21,  ...,  1,  1,  1],\n",
      "        [ 0,  8, 20,  ...,  1,  1,  1],\n",
      "        [ 0, 20, 16,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 0, 21, 19,  ...,  1,  1,  1],\n",
      "        [ 0, 20,  6,  ...,  1,  1,  1],\n",
      "        [ 0, 20, 10,  ...,  1,  1,  1]])\n",
      "torch.Size([12031, 884])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neil_\\AppData\\Local\\Temp\\ipykernel_4168\\1317320857.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_tokens_tensor = torch.tensor(batch_tokens)\n"
     ]
    }
   ],
   "source": [
    "# Use ESM-2's batch converter for tokneisation/padding\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "# Pull only the sequences in entire dataset for conversion\n",
    "train_data = [(i, train_dataset[i][\"seq\"]) for i in range(len(train_dataset))]\n",
    "# Tokenise sequences\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(train_data)\n",
    "\n",
    "# Make batch_labels a tensor\n",
    "#batch_labels = torch.tensor(batch_labels)\n",
    "#batch_tokens_tensor = torch.tensor(batch_tokens)\n",
    "batch_tokens_tensor = torch.tensor(batch_tokens).clone().detach()\n",
    "\n",
    "train_dataset = TensorDataset(batch_tokens_tensor)\n",
    "#train_dataset = TensorDataset(batch_labels, batch_tokens)\n",
    "\n",
    "# Check the tensor dimensions\n",
    "print(batch_tokens)\n",
    "print(batch_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "773ed0fc-cae2-40aa-8ef5-1c0f2c91e647",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([884])\n",
      "torch.Size([884])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x203360fe910>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(batch_tokens[0].shape)\n",
    "print(batch_tokens[1].shape)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6684f32-a8b2-4a3e-9ff7-41aafcd662cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters of the pretrained model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Last layer output fatures decided from ESM-2's layers?\n",
    "hidden_size = 320\n",
    "\n",
    "# Modify last layer for the regression task\n",
    "model.final_layer = nn.Sequential(\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(in_features=hidden_size, out_features=1),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "# Enable gradient computation for the parameters in the final_layer\n",
    "for param in model.final_layer.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5901bedf-a524-4581-bd6a-751134d5da8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Reshape tensors to 2D\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#inputs = inputs.unsqueeze(1)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 27\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# AssertionError: ---> 81 assert tokens.ndim == 2\u001b[39;00m\n\u001b[0;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, inputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ESM_contacts\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ESM_contacts\\lib\\site-packages\\esm\\model\\esm2.py:81\u001b[0m, in \u001b[0;36mESM2.forward\u001b[1;34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_contacts:\n\u001b[0;32m     79\u001b[0m     need_head_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     82\u001b[0m padding_mask \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39meq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)  \u001b[38;5;66;03m# B, T\u001b[39;00m\n\u001b[0;32m     84\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_scale \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens(tokens)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "# Clear cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#train_dataloader = DataLoader([batch_tokens], batch_size=8, shuffle=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        # Is there a padding issue for the shape problem, do the sequences need to be equal?\n",
    "        # Does (embed_tokens): Embedding(33, 320, padding_idx=1) from the model handle this issue?\n",
    "        inputs = batch #.to(device)\n",
    "        # Reshape tensors to 2D\n",
    "        #inputs = inputs.unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # AssertionError: ---> 81 assert tokens.ndim == 2\n",
    "        loss = loss_fn(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {average_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch+1} loss: {running_loss / len(train_dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe2511e-875a-4094-b3c4-5683151ac668",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8972fad4-f059-418e-a320-1ce629987bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "train_data = [(i, train_dataset[i][\"seq\"]) for i in range(len(train_dataset))]\n",
    "\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(train_data)\n",
    "\n",
    "\n",
    "batch_labels = torch.tensor(batch_labels)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(batch_labels, batch_tokens)\n",
    "\n",
    "print(batch_tokens)\n",
    "print(batch_tokens.shape)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {average_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch+1} loss: {running_loss / len(train_dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649555b-31ff-40bf-9d98-784d5aee8c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESM-2 tokeniser for data\n",
    "valid_data = [(i, valid_dataset[i][\"seq\"]) for i in range(len(valid_dataset))]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(valid_data)\n",
    "valid_dataloader = DataLoader(batch_tokens, batch_size=16, shuffle=False)\n",
    "\n",
    "# Set evaluation mode for dropout and batch normalisation\n",
    "model.eval()\n",
    "\n",
    "# Initialise lists   \n",
    "predictions = []\n",
    "true_contacts = []\n",
    "\n",
    "# Turn off gradients for evaluation\n",
    "with torch.no_grad():\n",
    "    for batch in valid_dataloader:\n",
    "        inputs = batch.to(device)  # Move batch to the specified device\n",
    "        outputs = model(inputs)\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "# Convert predictions and contact lists into tensors\n",
    "predictions = torch.tensor(predictions)\n",
    "true_contacts = torch.tensor(targets)\n",
    "\n",
    "# Evaluation metrics\n",
    "accuracy = accuracy_score(true_contacts, predictions)\n",
    "precision = precision_score(true_contacts, predictions)\n",
    "recall = recall_score(true_contacts, predictions)\n",
    "f1 = f1_score(true_contacts, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
