{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ddb0ac-b84d-4215-bdb2-3c52447f8fdf",
   "metadata": {},
   "source": [
    "# Clean up fine-tuning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c1d62-04a1-4066-9279-ea194b4f7865",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fair-esm\n",
    "# Clear cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "print(model)\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}, Size: {param.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c804a73-c9dd-437c-a7d5-08f0a4ab9008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from esm import pretrained\n",
    "from esm.data import ESMStructuralSplitDataset\n",
    "\n",
    "# Set device to use GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Download structural holdout datasets\n",
    "for split_level in ['family', 'superfamily', 'fold']:\n",
    "    for cv_partition in ['0', '1', '2', '3', '4']:\n",
    "        esm_structural_train = ESMStructuralSplitDataset(\n",
    "            split_level=split_level, \n",
    "            cv_partition=cv_partition, \n",
    "            split='train', \n",
    "            root_path = os.path.expanduser('~/.cache/torch/data/esm'),\n",
    "            download=True\n",
    "        )\n",
    "        esm_structural_valid = ESMStructuralSplitDataset(\n",
    "            split_level=split_level, \n",
    "            cv_partition=cv_partition, \n",
    "            split='valid', \n",
    "            root_path = os.path.expanduser('~/.cache/torch/data/esm'),\n",
    "            download=True\n",
    "        )\n",
    "\n",
    "esm_structural_train = ESMStructuralSplitDataset(\n",
    "    split_level='superfamily', \n",
    "    cv_partition='4', \n",
    "    split='train', \n",
    "    root_path = os.path.expanduser('~/.cache/torch/data/esm'),\n",
    ")\n",
    "\n",
    "esm_structural_valid = ESMStructuralSplitDataset(\n",
    "    split_level='superfamily', \n",
    "    cv_partition='4', \n",
    "    split='valid', \n",
    "    root_path = os.path.expanduser('~/.cache/torch/data/esm'),\n",
    ")\n",
    "\n",
    "elet = esm_structural_train[0]\n",
    "elev = esm_structural_valid[0]\n",
    "print(elet.keys()) \n",
    "print('sequence', elet['seq'])\n",
    "print('sequence', elev['seq'])\n",
    "\n",
    "# Check how many entries in dictionary\n",
    "print(len(esm_structural_train))\n",
    "print(len(esm_structural_valid))\n",
    "\n",
    "# Training dataset downloaded from ESMStructuralSplitDataset\n",
    "train_dataset = esm_structural_train\n",
    "valid_dataset = esm_structural_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd0ea89-675c-4b07-8cff-e9baa13bc461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters of the pretrained model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify only the last layer for regression task\n",
    "model.contact_head.regression = nn.Linear(in_features=120, out_features=1)\n",
    "\n",
    "# Set requires_grad=True only for the regression layer parameters to be trained\n",
    "for param in model.contact_head.regression.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6684f32-a8b2-4a3e-9ff7-41aafcd662cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "train_data = [(i, train_dataset[i][\"seq\"]) for i in range(len(train_dataset))]\n",
    "\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3429e842-65d6-4fff-8864-9da108e40021",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(batch_tokens, batch_size=16, shuffle=True)\n",
    "# TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'generator'>\n",
    "\n",
    "learning_rate = 0.001\n",
    "               \n",
    "# Maybe try \"AdamW\" with weight decay\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "               \n",
    "# Set objective function. Huber less sensitive to outliers than MSEloss, maybe try MSE as well?        \n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "      \n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "               \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "               \n",
    "    # Initialise loss for each epoch   \n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        # Sequence inputs - IndexError: too many indices for tensor of dimension 2\n",
    "        inputs = batch[\"batch_tokens\"]\n",
    "        # What are my targets in the dataset, is this just a placeholder to be populated, what key is this representing in my dataset?\n",
    "        targets = batch[\"???\"]\n",
    "        # Clear gradients for each epoch\n",
    "        optimizer.zero_grad()\n",
    "        # Output predictions for batch \n",
    "        outputs = model(inputs)\n",
    "        # Calculates Huber loss between predictions and true values\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        # Pool loss values from each batch\n",
    "        total_loss += loss.item()\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Updates last layer parameters to reduce loss\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Print loss per batch for the epoch \n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd82b3a-7da5-42f3-80fd-72a9bf9232cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Need to decide what keys in the dictionary are relevant\n",
    "valid_data = [(i, train_dataset[i][\"seq\"]) for i in range(len(valid_dataset))]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(valid_data)                           \n",
    "valid_dataloader = DataLoader(batch_tokens, batch_size=16, shuffle=False)\n",
    "\n",
    "# Set evaluation mode for dropout and batch normalisation\n",
    "model.eval()\n",
    "\n",
    "# Initialise lists               \n",
    "predictions = []\n",
    "true_contacts = []\n",
    "\n",
    "# Turn off gradients for evaluation\n",
    "with torch.no_grad(): \n",
    "    for batch in valid_dataloader:\n",
    "        inputs = batch[\"batch_tokens\"]\n",
    "        targets = batch[\"???\"]\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        predictions.extend(outputs.tolist())\n",
    "        true_contacts.extend(targets.tolist())\n",
    "\n",
    "# Convert lists into tensors\n",
    "predictions = torch.tensor(predictions)\n",
    "true_contacts = torch.tensor(true_contacts)\n",
    "\n",
    "# Evaluation metrics\n",
    "accuracy = accuracy_score(true_contacts, predictions)\n",
    "precision = precision_score(true_contacts, predictions)\n",
    "recall = recall_score(true_contacts, predictions)\n",
    "f1 = f1_score(true_contacts, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
